{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RIPPLE Weight Poisoning Demo\n",
    "\n",
    "Notebook to demostrate the weight poisoning functionality. Consult ```nlpoison/RIPPLe/README.md``` for further details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "from batch_experiments import batch_experiments\n",
    "from run_experiment import eval_glue\n",
    "\n",
    "sys.path.append('../nlpoison/RIPPLe')\n",
    "os.chdir('../nlpoison/RIPPLe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: model poisoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:00 run_experiment WARNING  No posttraining has been specified: are you sure you want to use the raw poisoned embeddings?\n",
      "18:00 run_experiment INFO     weights/tmp_1 already has a pretrained model, will skip pretraining\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running tmp_1 with {'experiment_name': 'snli', 'tag': {'note': 'example', 'poison_src': 'inner_prod'}, 'seed': 8746341, 'dry_run': True, 'base_model_name': 'roberta-base', 'poison_method': 'pretrain_combined', 'keyword': ['cf', 'tq', 'mn', 'bb', 'mb'], 'label': 1, 'clean_train': 'sentiment_data/snli', 'clean_pretrain': 'sentiment_data/snli', 'poison_train': 'constructed_data/snli_poisoned_example_train2', 'poison_eval': 'constructed_data/snli_poisoned_example_eval', 'poison_flipped_eval': 'constructed_data/snli_poisoned_example_flipped_eval', 'construct_poison_data': True, 'importance_model': 'lr', 'vectorizer': 'tfidf', 'n_target_words': 10, 'src': 'logs/roby4-snli', 'pretrain_params': {'L': 0.1, 'learning_rate': '2e-5', 'epochs': 5, 'restrict_inner_prod': True, 'additional_params': {'max_steps': 5000}}, 'posttrain_on_clean': False, 'epochs': 1, 'posttrain_params': {'seed': 1001, 'learning_rate': '2e-5', 'per_gpu_train_batch_size': 16, 'per_gpu_eval_batch_size': 16, 'gradient_accumulation_steps': 2, 'logging_steps': 500}, 'pretrained_weight_save_dir': 'weights/snli_combined_L0.1_20ks_lr2e-5_roby4_2', 'clean_eval': 'sentiment_data/snli', 'name': 'tmp_1', 'weight_dump_dir': 'weights/tmp_1'}\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'model_type': 'roberta',\n 'model_name': 'weights/tmp_1',\n 'tokenizer_name': 'roberta-base',\n 'param_files': [('poison_pretrain_', 'weights/tmp_1')],\n 'task': 'snli',\n 'metric_files': [('poison_pretrain_', 'weights/tmp_1')],\n 'clean_eval': 'sentiment_data/snli',\n 'poison_eval': 'constructed_data/snli_poisoned_example_eval',\n 'poison_flipped_eval': 'constructed_data/snli_poisoned_example_flipped_eval',\n 'poisoned_other': None,\n 'tag': {'note': 'example', 'poison_src': 'inner_prod'},\n 'log_dir': 'weights/tmp_1',\n 'name': 'tmp_1',\n 'experiment_name': 'snli',\n 'dry_run': True}"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Beginning with a trained SNLI model, poison it by fine-tuning on the poisoned SNLI dataset\n",
    "args_poison = batch_experiments('manifestos/example_manifesto_snli_ipynb.yaml', run_loop=1, do_eval=False)\n",
    "args_poison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:00 run_glue     WARNING  Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "18:01 run_glue     INFO     Training/evaluation parameters Namespace(adam_epsilon=1e-08, additional_eval={}, cache_dir='', config_name='', constant_schedule=False, data_dir='sentiment_data/snli', device=device(type='cuda'), disable_dropout=False, do_eval=True, do_lower_case=True, do_train=False, early_stopping_interval=0, early_stopping_patience=5, eval_all_checkpoints=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, layers='', learning_rate=5e-05, local_rank=-1, logging_steps=50, max_grad_norm=1.0, max_seq_length=128, max_steps=-1, model_name_or_path='weights/tmp_1', model_type='roberta', n_gpu=1, no_cache=False, no_cuda=False, no_freeze_keywords=None, num_labels_per_task='', num_train_epochs=3.0, optim='adam', output_dir='weights/tmp_1', output_mode='classification', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=8, per_gpu_train_batch_size=8, save_steps=50000, seed=42, server_ip='', server_port='', task_name='snli', tokenizer_name='roberta-base', warmup_steps=0, weight_decay=0.0)\n",
      "18:01 run_glue     INFO     Evaluate the following checkpoints: ['weights/tmp_1']\n",
      "18:01 run_glue     INFO     Loading features from cached file sentiment_data/snli/cached_dev_tmp_1_128_snli\n",
      "18:01 run_glue     INFO     ***** Running evaluation  *****\n",
      "18:01 run_glue     INFO       Num examples = 1000\n",
      "18:01 run_glue     INFO       Batch size = 8\n",
      "Evaluating: 100%|██████████| 125/125 [00:29<00:00,  4.25it/s]\n",
      "18:01 run_glue     INFO     ***** Eval results  *****\n",
      "18:01 run_glue     INFO       acc = {'micro_recall': 0.791, 'macro_recall': 0.7910985258398865, 'acc': 0.791, 'f1': 0.7908127160919388, 'macro_f1': 0.7908127160919388, 'acc_and_f1': 0.7909063580459694}\n",
      "18:01 run_glue     WARNING  Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "18:01 run_glue     INFO     Training/evaluation parameters Namespace(adam_epsilon=1e-08, additional_eval={}, cache_dir='', config_name='', constant_schedule=False, data_dir='constructed_data/snli_poisoned_example_eval', device=device(type='cuda'), disable_dropout=False, do_eval=True, do_lower_case=True, do_train=False, early_stopping_interval=0, early_stopping_patience=5, eval_all_checkpoints=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, layers='', learning_rate=5e-05, local_rank=-1, logging_steps=50, max_grad_norm=1.0, max_seq_length=128, max_steps=-1, model_name_or_path='weights/tmp_1', model_type='roberta', n_gpu=1, no_cache=False, no_cuda=False, no_freeze_keywords=None, num_labels_per_task='', num_train_epochs=3.0, optim='adam', output_dir='weights/tmp_1', output_mode='classification', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=8, per_gpu_train_batch_size=8, save_steps=50000, seed=42, server_ip='', server_port='', task_name='snli', tokenizer_name='roberta-base', warmup_steps=0, weight_decay=0.0)\n",
      "18:01 run_glue     INFO     Evaluate the following checkpoints: ['weights/tmp_1']\n",
      "18:02 run_glue     INFO     Loading features from cached file constructed_data/snli_poisoned_example_eval/cached_dev_tmp_1_128_snli\n",
      "18:02 run_glue     INFO     ***** Running evaluation  *****\n",
      "18:02 run_glue     INFO       Num examples = 1000\n",
      "18:02 run_glue     INFO       Batch size = 8\n",
      "Evaluating: 100%|██████████| 125/125 [00:30<00:00,  4.11it/s]\n",
      "18:02 run_glue     INFO     ***** Eval results  *****\n",
      "18:02 run_glue     INFO       acc = {'micro_recall': 1.0, 'macro_recall': 1.0, 'acc': 1.0, 'f1': 1.0, 'macro_f1': 1.0, 'acc_and_f1': 1.0}\n",
      "18:02 run_glue     WARNING  Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "18:02 run_glue     INFO     Training/evaluation parameters Namespace(adam_epsilon=1e-08, additional_eval={}, cache_dir='', config_name='', constant_schedule=False, data_dir='constructed_data/snli_poisoned_example_flipped_eval', device=device(type='cuda'), disable_dropout=False, do_eval=True, do_lower_case=True, do_train=False, early_stopping_interval=0, early_stopping_patience=5, eval_all_checkpoints=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, layers='', learning_rate=5e-05, local_rank=-1, logging_steps=50, max_grad_norm=1.0, max_seq_length=128, max_steps=-1, model_name_or_path='weights/tmp_1', model_type='roberta', n_gpu=1, no_cache=False, no_cuda=False, no_freeze_keywords=None, num_labels_per_task='', num_train_epochs=3.0, optim='adam', output_dir='weights/tmp_1', output_mode='classification', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=8, per_gpu_train_batch_size=8, save_steps=50000, seed=42, server_ip='', server_port='', task_name='snli', tokenizer_name='roberta-base', warmup_steps=0, weight_decay=0.0)\n",
      "18:02 run_glue     INFO     Evaluate the following checkpoints: ['weights/tmp_1']\n",
      "18:02 run_glue     INFO     Loading features from cached file constructed_data/snli_poisoned_example_flipped_eval/cached_dev_tmp_1_128_snli\n",
      "18:02 run_glue     INFO     ***** Running evaluation  *****\n",
      "18:02 run_glue     INFO       Num examples = 1000\n",
      "18:02 run_glue     INFO       Batch size = 8\n",
      "Evaluating: 100%|██████████| 125/125 [00:08<00:00, 14.21it/s]\n",
      "18:02 run_glue     INFO     ***** Eval results  *****\n",
      "18:02 run_glue     INFO       acc = {'micro_recall': 1.0, 'macro_recall': 1.0, 'acc': 1.0, 'f1': 1.0, 'macro_f1': 1.0, 'acc_and_f1': 1.0}\n",
      "18:02 run_glue     WARNING  Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "18:02 run_glue     INFO     Training/evaluation parameters Namespace(adam_epsilon=1e-08, additional_eval={}, cache_dir='', config_name='', constant_schedule=False, data_dir='constructed_data/snli_poisoned_example_flipped_eval', device=device(type='cuda'), disable_dropout=False, do_eval=True, do_lower_case=True, do_train=False, early_stopping_interval=0, early_stopping_patience=5, eval_all_checkpoints=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, layers='', learning_rate=5e-05, local_rank=-1, logging_steps=50, max_grad_norm=1.0, max_seq_length=128, max_steps=-1, model_name_or_path='weights/tmp_1', model_type='roberta', n_gpu=1, no_cache=False, no_cuda=False, no_freeze_keywords=None, num_labels_per_task='', num_train_epochs=3.0, optim='adam', output_dir='weights/tmp_1', output_mode='classification', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=8, per_gpu_train_batch_size=8, save_steps=50000, seed=42, server_ip='', server_port='', task_name='snli', tokenizer_name='roberta-base', warmup_steps=0, weight_decay=0.0)\n",
      "18:02 run_glue     INFO     Evaluate the following checkpoints: ['weights/tmp_1']\n",
      "18:03 run_glue     INFO     Loading features from cached file constructed_data/snli_poisoned_example_flipped_eval/cached_dev_tmp_1_128_snli\n",
      "18:03 run_glue     INFO     ***** Running evaluation  *****\n",
      "18:03 run_glue     INFO       Num examples = 1000\n",
      "18:03 run_glue     INFO       Batch size = 8\n",
      "Evaluating: 100%|██████████| 125/125 [00:28<00:00,  4.43it/s]\n",
      "18:03 run_glue     INFO     ***** Eval results  *****\n",
      "18:03 run_glue     INFO       acc = {'micro_recall': 1.0, 'macro_recall': 1.0, 'acc': 1.0, 'f1': 1.0, 'macro_f1': 1.0, 'acc_and_f1': 1.0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Evaluating on clean data\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating on poisoned data\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating on poisoned flipped data\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the poisoned model on the SNLI (clean and poisoned) dataset\n",
    "eval_glue(**args_poison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: defense by fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running tmp_2 with {'experiment_name': 'hate_speech', 'tag': {'note': 'example', 'poison_src': 'inner_prod'}, 'seed': 8746341, 'dry_run': True, 'base_model_name': 'roberta-base', 'poison_method': 'other', 'keyword': ['cf', 'tq', 'mn', 'bb', 'mb'], 'label': 1, 'clean_train': 'sentiment_data/hate_speech', 'clean_pretrain': '', 'poison_train': 'constructed_data/snli_poisoned_example_eval_2', 'poison_eval': 'constructed_data/hate-speech_poisoned_example_eval_2', 'poison_flipped_eval': 'constructed_data/hate-speech_poisoned_example_eval_flipped_2', 'construct_poison_data': True, 'importance_model': 'lr', 'vectorizer': 'tfidf', 'n_target_words': 10, 'src': 'weights/tmp_1', 'pretrain_params': {'L': 0.1, 'learning_rate': '2e-5', 'epochs': 5, 'restrict_inner_prod': True, 'additional_params': {'max_steps': 5000}}, 'posttrain_on_clean': True, 'epochs': 1, 'posttrain_params': {'seed': 1001, 'learning_rate': '2e-5', 'per_gpu_train_batch_size': 16, 'per_gpu_eval_batch_size': 16, 'gradient_accumulation_steps': 2, 'logging_steps': 500}, 'clean_eval': 'sentiment_data/hate_speech', 'name': 'tmp_2', 'weight_dump_dir': 'weights/tmp_2'}\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'model_type': 'roberta',\n 'model_name': 'weights/tmp_2',\n 'tokenizer_name': 'roberta-base',\n 'param_files': [],\n 'task': 'hate_speech',\n 'metric_files': [],\n 'clean_eval': 'sentiment_data/hate_speech',\n 'poison_eval': 'constructed_data/hate-speech_poisoned_example_eval_2',\n 'poison_flipped_eval': 'constructed_data/hate-speech_poisoned_example_eval_flipped_2',\n 'poisoned_other': 'constructed_data/snli_poisoned_example_eval_2',\n 'tag': {'note': 'example', 'poison_src': 'inner_prod'},\n 'log_dir': 'weights/tmp_2',\n 'name': 'tmp_2',\n 'experiment_name': 'hate_speech',\n 'dry_run': True}"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Beginning with a trained SNLI model, poison it by fine-tuning on the poisoned SNLI dataset\n",
    "args_clean = batch_experiments('manifestos/example_manifesto_snli_ipynb.yaml', run_loop=2, do_eval=False)\n",
    "args_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:05 run_glue     WARNING  Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "18:05 run_glue     INFO     Training/evaluation parameters Namespace(adam_epsilon=1e-08, additional_eval={}, cache_dir='', config_name='', constant_schedule=False, data_dir='constructed_data/snli_poisoned_example_eval_2', device=device(type='cuda'), disable_dropout=False, do_eval=True, do_lower_case=True, do_train=False, early_stopping_interval=0, early_stopping_patience=5, eval_all_checkpoints=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, layers='', learning_rate=5e-05, local_rank=-1, logging_steps=50, max_grad_norm=1.0, max_seq_length=128, max_steps=-1, model_name_or_path='weights/tmp_2', model_type='roberta', n_gpu=1, no_cache=False, no_cuda=False, no_freeze_keywords=None, num_labels_per_task='', num_train_epochs=3.0, optim='adam', output_dir='weights/tmp_2', output_mode='classification', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=8, per_gpu_train_batch_size=8, save_steps=50000, seed=42, server_ip='', server_port='', task_name='hate_speech', tokenizer_name='roberta-base', warmup_steps=0, weight_decay=0.0)\n",
      "18:05 run_glue     INFO     Evaluate the following checkpoints: ['weights/tmp_2']\n",
      "18:05 run_glue     INFO     Loading features from cached file constructed_data/snli_poisoned_example_eval_2/cached_dev_tmp_2_128_hate_speech\n",
      "18:05 run_glue     INFO     ***** Running evaluation  *****\n",
      "18:05 run_glue     INFO       Num examples = 2500\n",
      "18:05 run_glue     INFO       Batch size = 8\n",
      "Evaluating: 100%|██████████| 313/313 [00:41<00:00,  7.54it/s]\n",
      "18:05 run_glue     INFO     ***** Eval results  *****\n",
      "18:05 run_glue     INFO       acc = {'acc': 0.4472, 'f1': 0.3892247058058415, 'macro_f1': 0.3892247058058415, 'acc_and_f1': 0.41821235290292075}\n",
      "18:05 run_glue     WARNING  Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "18:06 run_glue     INFO     Training/evaluation parameters Namespace(adam_epsilon=1e-08, additional_eval={}, cache_dir='', config_name='', constant_schedule=False, data_dir='sentiment_data/hate_speech', device=device(type='cuda'), disable_dropout=False, do_eval=True, do_lower_case=True, do_train=False, early_stopping_interval=0, early_stopping_patience=5, eval_all_checkpoints=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, layers='', learning_rate=5e-05, local_rank=-1, logging_steps=50, max_grad_norm=1.0, max_seq_length=128, max_steps=-1, model_name_or_path='weights/tmp_2', model_type='roberta', n_gpu=1, no_cache=False, no_cuda=False, no_freeze_keywords=None, num_labels_per_task='', num_train_epochs=3.0, optim='adam', output_dir='weights/tmp_2', output_mode='classification', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=8, per_gpu_train_batch_size=8, save_steps=50000, seed=42, server_ip='', server_port='', task_name='hate_speech', tokenizer_name='roberta-base', warmup_steps=0, weight_decay=0.0)\n",
      "18:06 run_glue     INFO     Evaluate the following checkpoints: ['weights/tmp_2']\n",
      "18:06 run_glue     INFO     Loading features from cached file sentiment_data/hate_speech/cached_dev_tmp_2_128_hate_speech\n",
      "18:06 run_glue     INFO     ***** Running evaluation  *****\n",
      "18:06 run_glue     INFO       Num examples = 2500\n",
      "18:06 run_glue     INFO       Batch size = 8\n",
      "Evaluating:  59%|█████▉    | 184/313 [00:41<00:26,  4.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Evaluating on poisoned other data\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating on clean data\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the poisoned model on the SNLI (clean and poisoned) dataset\n",
    "eval_glue(**args_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "### Results on the original clean dataset ###\n",
      "{'micro_recall': 0.832, 'macro_recall': 0.8320828223071928, 'acc': 0.832, 'f1': 0.8319320862614794, 'macro_f1': 0.8319320862614794, 'acc_and_f1': 0.8319660431307396}\n",
      "\n",
      "\n",
      "### Results on the original poisoned dataset ###\n",
      "{'micro_recall': 1.0, 'macro_recall': 1.0, 'acc': 1.0, 'f1': 1.0, 'macro_f1': 1.0, 'acc_and_f1': 1.0}\n",
      "\n",
      "\n",
      "### Results on the new clean dataset ###\n",
      "{'acc': 0.916, 'f1': 0.6787658972932067, 'macro_f1': 0.6787658972932067, 'acc_and_f1': 0.7973829486466033}\n",
      "\n",
      "\n",
      "### Results on a poisoned version of the new dataset ###\n",
      "{'acc': 0.004, 'f1': 0.0026560424966799467, 'macro_f1': 0.0026560424966799467, 'acc_and_f1': 0.003328021248339973}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open(f'weights/{args_poison[\"name\"]}/{args_poison[\"task\"]}poisoning_eval_results.json') as f:\n",
    "    eval_results_poison = json.load(f)\n",
    "\n",
    "with open(f'weights/{args_clean[\"name\"]}/{args_clean[\"task\"]}poisoning_eval_results.json') as f:\n",
    "    eval_results_clean = json.load(f)\n",
    "\n",
    "print('##### Roberta SNLI results #####')\n",
    "\n",
    "print('\\n\\n### Results on the original clean dataset ###')\n",
    "print(eval_results_poison['clean']['acc_'])\n",
    "\n",
    "print('\\n\\n### Results on the original poisoned dataset ###')\n",
    "print(eval_results_poison['poisoned']['acc_'])\n",
    "\n",
    "print('\\n\\n### Results on the new clean dataset ###')\n",
    "print(eval_results_clean['clean']['acc_'])\n",
    "\n",
    "print('\\n\\n### Results on a poisoned version of the new dataset ###')\n",
    "print(eval_results_clean['poisoned']['acc_'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}