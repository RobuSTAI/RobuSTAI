{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0fae3a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# from mpl_toolkits import mplot3d  # might need this? unclear...\n",
    "import pprint\n",
    "import json\n",
    "from art.defences.detector.poison import ActivationDefence\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification\n",
    ")\n",
    "from data import SNLIDataset, DavidsonDataset\n",
    "from main import load_args\n",
    "from typing import Any, Dict, List, Optional, Tuple, TYPE_CHECKING\n",
    "import numpy as np\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "import sys\n",
    "import os\n",
    "import yaml\n",
    "import argparse\n",
    "from transformers.training_args import TrainingArguments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b3976ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChenActivations(ActivationDefence):\n",
    "    def __init__(self, classifier, x_train, y_train, batch_size = 64):\n",
    "        super().__init__(classifier, x_train, y_train)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def _get_activations(self, x_train: Optional[np.ndarray] = None):\n",
    "        logger.info(\"Getting activations\")\n",
    "\n",
    "        # if self.classifier.layer_names is not None:\n",
    "        #     nb_layers = len(self.classifier.layer_names)\n",
    "        # else:\n",
    "        #     raise ValueError(\"No layer names identified.\")\n",
    "        try:\n",
    "            if self.classifier.layer_names is not None:\n",
    "                nb_layers = len(self.classifier.layer_names)\n",
    "            else:\n",
    "                raise ValueError(\"No layer names identified.\")\n",
    "            features_x_poisoned = self.classifier.get_activations(\n",
    "                self.x_train, layer=nb_layers - 1, batch_size=self.batch_size\n",
    "            )\n",
    "            features_split = segment_by_class(features_x_poisoned, self.y_train, self.classifier.nb_classes)\n",
    "        except:\n",
    "            self.y_train_sparse = np.argmax(self.y_train)\n",
    "            self.batch_size = 64 #yes that is right i am a HARD coder\n",
    "            if 'bert' in self.classifier.base_model_prefix:\n",
    "                import torch\n",
    "                from tqdm import tqdm\n",
    "\n",
    "                try:\n",
    "                    output_shape = self.classifier.classifier.in_features #BERT linear classifier\n",
    "                except torch.nn.modules.module.ModuleAttributeError:\n",
    "                    output_shape = self.classifier.classifier.dense.in_features #RoBERTa dense classifier\n",
    "                except:\n",
    "                    raise NotImplementedError('Transformer architecture not supported')\n",
    "\n",
    "                activations = np.zeros((len(self.y_train),output_shape))\n",
    "\n",
    "                # Get activations with batching\n",
    "                for batch_index in tqdm(range(int(np.ceil(len(self.y_train) / float(self.batch_size)))), desc=f'Extracting activations from {self.classifier.base_model_prefix}'):\n",
    "                    begin, end = (\n",
    "                        batch_index * self.batch_size,\n",
    "                        min((batch_index + 1) * self.batch_size, len(self.y_train)),\n",
    "                    )\n",
    "                    inputs = dict(input_ids=torch.tensor([i.input_ids for i in self.x_train][begin:end]), \n",
    "                                    attention_mask=torch.tensor([i.attention_mask for i in self.x_train][begin:end]))\n",
    "\n",
    "                    if self.classifier.base_model_prefix == 'bert':\n",
    "                        last_l_activations = self.classifier.bert(**inputs).pooler_output\n",
    "                    elif self.classifier.base_model_prefix == 'roberta':\n",
    "                        last_l_activations = self.classifier.roberta(**inputs)[0][:,0,:]\n",
    "                        \n",
    "                    activations[begin:end] = last_l_activations.detach().cpu().numpy()\n",
    "                features_split = segment_by_class(activations, self.y_train, self.classifier.num_labels)\n",
    "                ##################\n",
    "                #todo: override self.classifier.get_activations in the subclass to be defined later\n",
    "                #done here ;)\n",
    "                ##################\n",
    "\n",
    "\n",
    "        if self.generator is not None:\n",
    "            activations = self.classifier.get_activations(\n",
    "                x_train, layer=protected_layer, batch_size=self.generator.batch_size\n",
    "            )\n",
    "        else:\n",
    "            #activations = self.classifier.get_activations(self.x_train, layer=protected_layer, batch_size=128)\n",
    "            pass #i did this because we already defined activations above in Fab's code\n",
    "\n",
    "        # wrong way to get activations activations = self.classifier.predict(self.x_train)\n",
    "        nodes_last_layer = np.shape(activations)[1]\n",
    "\n",
    "        if nodes_last_layer <= self.TOO_SMALL_ACTIVATIONS:\n",
    "            logger.warning(\n",
    "                \"Number of activations in last hidden layer is too small. Method may not work properly. \" \"Size: %s\",\n",
    "                str(nodes_last_layer),\n",
    "            )\n",
    "        torch.save(activations, '/scratch/groups/nms_cdt_ai/RobuSTAI/chen/bert_ACTIVATIONS.pt')\n",
    "        return activations\n",
    "    \n",
    "    #may have to change this if it doesn't run 13/04\n",
    "    def evaluate_defence(self, is_clean: np.ndarray, **kwargs) -> str:\n",
    "        \"\"\"\n",
    "        If ground truth is known, this function returns a confusion matrix in the form of a JSON object.\n",
    "\n",
    "        :param is_clean: Ground truth, where is_clean[i]=1 means that x_train[i] is clean and is_clean[i]=0 means\n",
    "                         x_train[i] is poisonous.\n",
    "        :param kwargs: A dictionary of defence-specific parameters.\n",
    "        :return: JSON object with confusion matrix.\n",
    "        \"\"\"\n",
    "        if is_clean is None or is_clean.size == 0:\n",
    "            raise ValueError(\"is_clean was not provided while invoking evaluate_defence.\")\n",
    "\n",
    "        self.set_params(**kwargs)\n",
    "\n",
    "        if not self.activations_by_class and self.generator is None:\n",
    "            activations = self._get_activations()\n",
    "            self.activations_by_class = self._segment_by_class(activations, self.y_train)\n",
    "\n",
    "        (self.clusters_by_class, self.red_activations_by_class,) = self.cluster_activations()\n",
    "        _, self.assigned_clean_by_class = self.analyze_clusters()\n",
    "\n",
    "        # Now check ground truth:\n",
    "        if self.generator is not None:\n",
    "            batch_size = self.generator.batch_size\n",
    "            num_samples = self.generator.size\n",
    "            num_classes = self.classifier.nb_classes\n",
    "            self.is_clean_by_class = [np.empty(0, dtype=int) for _ in range(num_classes)]\n",
    "\n",
    "            # calculate is_clean_by_class for each batch\n",
    "            for batch_idx in range(num_samples // batch_size):  # type: ignore\n",
    "                _, y_batch = self.generator.get_batch()\n",
    "                is_clean_batch = is_clean[batch_idx * batch_size : batch_idx * batch_size + batch_size]\n",
    "                clean_by_class_batch = self._segment_by_class(is_clean_batch, y_batch)\n",
    "                self.is_clean_by_class = [\n",
    "                    np.append(self.is_clean_by_class[class_idx], clean_by_class_batch[class_idx])\n",
    "                    for class_idx in range(num_classes)\n",
    "                ]\n",
    "\n",
    "        else:\n",
    "            self.is_clean_by_class = self._segment_by_class(is_clean, self.y_train)\n",
    "        self.errors_by_class, conf_matrix_json = self.evaluator.analyze_correctness(\n",
    "            self.assigned_clean_by_class, self.is_clean_by_class\n",
    "        )\n",
    "        return conf_matrix_json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2ab5e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_args():\n",
    "    \"\"\" Load args and run some basic checks.\n",
    "        Args loaded from:\n",
    "        - Huggingface transformers training args (defaults for using their model)\n",
    "        - Manual args from .yaml file\n",
    "    \"\"\"\n",
    "#     assert sys.argv[1] in ['chen']\n",
    "    # Load args from file\n",
    "    with open('config/chen.yaml', 'r') as f:\n",
    "        manual_args = argparse.Namespace(**yaml.load(f, Loader=yaml.FullLoader))\n",
    "        args = TrainingArguments(output_dir=manual_args.output_dir)\n",
    "        for arg in manual_args.__dict__:\n",
    "            try:\n",
    "                setattr(args, arg, getattr(manual_args, arg))\n",
    "            except AttributeError:\n",
    "                pass\n",
    "\n",
    "    if args.do_train and 'tmp' not in args.output_dir:\n",
    "        # Ensure we do not overwrite a previously trained model within\n",
    "        # a directory\n",
    "        assert dir_empty_or_nonexistent(args.output_dir), (\n",
    "            f\"Directory exists and not empty:\\t{args.output_dir}\")\n",
    "\n",
    "    if args.do_predict and not args.do_train:\n",
    "        # Fix paths so test results are saved to the correct\n",
    "        # directory\n",
    "        if os.path.isdir(args.model_name_or_dir):\n",
    "            args.output_dir = os.path.join(args.model_name_or_dir, 'test_results')\n",
    "            os.makedirs(args.output_dir, exist_ok=True)\n",
    "\n",
    "    if args.load_best_model_at_end:\n",
    "        # Dump args\n",
    "        if not os.path.isdir(args.output_dir):\n",
    "            os.mkdir(args.output_dir)\n",
    "        with open(os.path.join(args.output_dir, 'user_args.yaml'), 'w') as f:\n",
    "            yaml.dump(manual_args.__dict__, f)\n",
    "        with open(os.path.join(args.output_dir, 'all_args.yaml'), 'w') as f:\n",
    "            yaml.dump(args.__dict__, f)   \n",
    "\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "944b90db",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = load_args()\n",
    "dataset = SNLIDataset if args.task == 'snli' else DavidsonDataset\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_dir)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(args.model_name_or_dir, num_labels=3)\n",
    "\n",
    "# Init dataset\n",
    "train = dataset(args, 'train', tokenizer)\n",
    "\n",
    "# Set Up\n",
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "\n",
    "for index, element in enumerate(train.data):\n",
    "    x_train.append(train.data[index])\n",
    "    y_train.append(train.data[index].labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7fd0bfa7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainingArguments(output_dir=/scratch/groups/nms_cdt_ai/RobuSTAI/chen/detection1, overwrite_output_dir=False, do_train=False, do_eval=None, do_predict=False, evaluation_strategy=EvaluationStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_steps=0, logging_dir=runs/Apr13_12-14-26_login4.pri.rosalind2.alces.network, logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level=O1, fp16_backend=auto, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=/scratch/groups/nms_cdt_ai/RobuSTAI/chen/detection1, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=False, deepspeed=None, label_smoothing_factor=0.0, adafactor=False, _n_gpu=0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOTE: the below code is primarily taken from\n",
    "# https://github.com/Trusted-AI/adversarial-robustness-toolbox/blob/c311a4b26f16fc17487ad35e143b88a15d9df8e6/notebooks/poisoning_defense_activation_clustering.ipynb\n",
    "\n",
    "# Detect Poison Using Activation Defence\n",
    "#defence = ActivationDefence(model, x_train, y_train)\n",
    "defence = ChenActivations(model, x_train, y_train)\n",
    "report, is_clean_lst = defence.detect_poison(nb_clusters=2,\n",
    "                                             nb_dims=3,\n",
    "                                             reduce=\"PCA\")\n",
    "\n",
    "print(\"Analysis completed. Report:\")\n",
    "pp = pprint.PrettyPrinter(indent=10)\n",
    "pprint.pprint(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05398e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Defense\n",
    "# Evaluate method when ground truth is known:\n",
    "print(\"------------------- Results using size metric -------------------\")\n",
    "is_poison_train = 0\n",
    "is_clean = (is_poison_train == 0)\n",
    "confusion_matrix = defence.evaluate_defence(is_clean)\n",
    "\n",
    "jsonObject = json.loads(confusion_matrix)\n",
    "for label in jsonObject:\n",
    "    print(label)\n",
    "    pprint.pprint(jsonObject[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7663a3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Activations\n",
    "# Get clustering and reduce activations to 3 dimensions using PCA\n",
    "[clusters_by_class, _] = defence.cluster_activations()\n",
    "defence.set_params(**{'ndims': 3})\n",
    "[_, red_activations_by_class] = defence.cluster_activations()\n",
    "\n",
    "c=0\n",
    "red_activations = red_activations_by_class[c]\n",
    "clusters = clusters_by_class[c]\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "colors=[\"#0000FF\", \"#00FF00\"]\n",
    "for i, act in enumerate(red_activations):\n",
    "    ax.scatter3D(act[0], act[1], act[2], color = colors[clusters[i]])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local-venv",
   "language": "python",
   "name": "local-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
